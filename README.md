# Data Car Wash ðŸš—âœ¨

A Python data processing pipeline that takes dirty data from KoBoToolbox and other sources, cleans it, and outputs pristine, organized data.

## Overview

Data Car Wash is a modular data processing pipeline with the following stages:

1. **Load** - Import data from KoBoToolbox, CSV, Excel, or JSON files
2. **Normalize** - Standardize formats, handle missing values, clean text
3. **Deduplicate** - Remove duplicate records based on configurable rules
4. **Organize** - Group, sort, and filter data based on your requirements
5. **Save** - Export as a zip file with optional encryption

## Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Install the package in development mode
pip install -e .
```

## Project Structure

```
datacarwashhospicetororo/
â”œâ”€â”€ datacarwash/              # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py                # Command-line interface
â”‚   â”œâ”€â”€ pipeline.py           # Main pipeline orchestration
â”‚   â”œâ”€â”€ kobo.py               # KoBoToolbox integration
â”‚   â”œâ”€â”€ normalization.py      # Data normalization
â”‚   â”œâ”€â”€ deduplication.py      # Duplicate removal
â”‚   â”œâ”€â”€ organization.py       # Data organization
â”‚   â”œâ”€â”€ encryption.py         # Encryption/decryption
â”‚   â””â”€â”€ utils/                # Utility modules
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ zipper.py
â”œâ”€â”€ config/                   # Configuration files
â”‚   â””â”€â”€ default.yaml
â”œâ”€â”€ data/                     # Data directories
â”‚   â”œâ”€â”€ input/               # Input data
â”‚   â”œâ”€â”€ output/              # Processed output
â”‚   â””â”€â”€ temp/                # Temporary files
â”œâ”€â”€ tests/                    # Test suite
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

## Usage

### Command Line Interface

The main command is `datacarwash`:

```bash
# Basic usage - process data
datacarwash wash -i data/input/my_data.csv -o data/output/clean_data.zip

# With encryption
datacarwash wash -i data/input/my_data.csv -o data/output/clean_data.zip \
  --encrypt --password mypassword

# With custom configuration
datacarwash wash -i data/input/my_data.csv -o data/output/clean_data.zip \
  -c config/my_config.yaml

# Verbose logging
datacarwash wash -i data/input/my_data.csv -o data/output/clean_data.zip -v
```

### Fetch from KoBoToolbox

```bash
datacarwash fetch-kobo \
  --url https://kobo.example.com \
  --token YOUR_API_TOKEN \
  --form-id YOUR_FORM_ID \
  --output data/input/kobo_data.json
```

### Decrypt Files

```bash
datacarwash decrypt \
  -f data/output/encrypted.zip \
  -o data/output/decrypted.zip \
  --password mypassword
```

## Configuration

Create a YAML configuration file to customize the pipeline behavior:

```yaml
# config/my_config.yaml

normalization:
  missing_value_strategy: keep  # keep, drop_rows, or drop_columns
  date_columns:
    - submission_date
    - created_at
  custom_rules:
    status:
      lowercase: true
    category:
      value_map:
        old_value: new_value

deduplication:
  strategy: key_columns  # all_columns or key_columns
  key_columns:
    - participant_id
    - submission_date
  keep: first  # first, last, or false

organization:
  sort:
    columns:
      - category
      - date
    ascending: true
  grouping:
    column: category  # Group into separate files by this column
  filters:
    - column: status
      operator: equals
      value: complete
```

## Python API

You can also use the pipeline programmatically:

```python
from pathlib import Path
from datacarwash.pipeline import DataCarWashPipeline

# Initialize pipeline
pipeline = DataCarWashPipeline(config_path=Path("config/my_config.yaml"))

# Run the pipeline
pipeline.run(
    input_path=Path("data/input/my_data.csv"),
    output_path=Path("data/output/clean_data.zip"),
    encrypt=True,
    password="mypassword"
)
```

## Features

- âœ… **Multiple Input Formats**: CSV, Excel, JSON
- âœ… **KoBoToolbox Integration**: Direct API access to fetch form submissions
- âœ… **Flexible Normalization**: Configurable rules for data standardization
- âœ… **Smart Deduplication**: Remove duplicates based on all columns or specific keys
- âœ… **Data Organization**: Group, sort, and filter data
- âœ… **Encryption**: Secure your data with password-based encryption
- âœ… **Zip Output**: Clean data packaged in a compressed archive
- âœ… **Full Control**: Extensive configuration options for complete control

## Development

### Running Tests

```bash
pytest tests/
```

### Adding Custom Processing Steps

The pipeline is designed to be extensible. You can add custom processing steps by:

1. Creating a new module in the `datacarwash/` directory
2. Implementing your processing logic
3. Integrating it into the pipeline in `pipeline.py`

## License

MIT License

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.